{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5 内存\n",
    "\n",
    "- Date: 2015-7-6\n",
    "- 杨攀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "### [1. CPU & GPU hardware](#part1)\n",
    "\n",
    "### [2. Introduction to CUDA](#part2)\n",
    "\n",
    "### [3. CUDA with C/C++](#part3)\n",
    "\n",
    "### [4. CUDA with Python](#part4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part1\"></a>\n",
    "# 一. CPU & GPU hardware\n",
    "\n",
    "## 1.1 CPU\n",
    "\n",
    "![figure 1. workstation architecture](http://7xk3b0.com1.z0.glb.clouddn.com/cpu.jpg?imageView2/2/w/700/q/75)\n",
    "\n",
    "### video data flow for example: \n",
    "- Source video from disk to CPU memory(through I/O hub)\n",
    "- CPU decode video, stored in CPU memory\n",
    "- GPU pull the data from CPU memory to GPU memory(through I/O hub)\n",
    "- GPU processing on the video frame (e.g. rendering), stored on GPU memory\n",
    "- Preview on displays or transfered to CPU memory for encode or storage\n",
    "\n",
    "### CPU 主要负责逻辑性较强的运算，其设计目标是使得执行单元能够以低延迟获得数据和指令，因此采用了复杂的控制逻辑和分支预测。\n",
    "\n",
    "## 1.2 GPU\n",
    "\n",
    "![figure 2. gpu architecture](http://7xk3b0.com1.z0.glb.clouddn.com/gpu.jpg?imageView2/2/w/700/q/75)\n",
    "\n",
    "### part explanation of gpu\n",
    "- A host interface: connect to PCIe bus, communicates with host CPU (Copy Engine)\n",
    "- Giga Thread: the thread scheduler, creates threads in hardware and distribute work to cores\n",
    "- DRAM: dynamic random access memory\n",
    "\n",
    "### GPU 设计目标时在有限的面积实现很强的计算能力和很高的存储器带宽。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part2\"></a>\n",
    "# 二. Introduction to CUDA\n",
    "\n",
    "## 2.1 CUDA 简介\n",
    "\n",
    "### 2.1.1 GPGPU \n",
    "- General Purpose Graphics Processing Unit\n",
    "- APIs： \n",
    "     CUDA(for nvidia GPUs only， 2007)\n",
    "     OpenCL\n",
    "     OpenACC\n",
    "### 2.1.2 what is cuda? [nvidia blogs]( http://blogs.nvidia.com/blog/2012/09/10/what-is-cuda-2/)\n",
    "- Compute Unified Device Architecture, 计算统一设备架构\n",
    "\n",
    "### 2.1.3 setup cuda\n",
    "- ubuntu cuda, [install](http://wiki.ubuntu.org.cn/NVIDIA)\n",
    "- pycuda：provides a Python interface to nvidia's CUDA API\n",
    "\n",
    "### 2.1.4 Simple Processing Flow\n",
    "- Copy input data from cpu memory to GPU memory\n",
    "- Load GPU program and execute, caching data on chip for performance\n",
    "- Copy results back to CPU memory \n",
    "\n",
    "### 2.1.5 运行时API 和驱动API\n",
    "- 两者都提供了实现设备管理，上下文管理，存储器管理，执行控制等应用程序接口\n",
    "- runtime API 在 driver API 基础上进行了封装，隐藏了一些细节，编程更方便，我们例子中使用的是runtime API\n",
    "- runtime API 函数以 cuda 为前缀， driver API以 cu为前缀\n",
    "\n",
    "\n",
    "\n",
    "## 2.2 CUDA 编程模型\n",
    "- 主机（cpu）与设备（gpu）\n",
    "- kernel 函数： 运行在gpu上的cuda并行计算函数\n",
    "- 一个完整的cuda函数由一系列的设备端kernel函数__并行__步骤和主机端的__串行__步骤共同组成\n",
    "- 一个kernel函数中存在两个层次的并行，Grid中的block之间的并行和block中的thread间并行，两层模型是cuda最重要的创新之一\n",
    "![figure 3. cuda programming model](http://7xk3b0.com1.z0.glb.clouddn.com/cudamodel.png?imageView2/2/w/700/q/75)\n",
    "\n",
    "\\--------------------------------------------------------------------------------------------------------\n",
    "\n",
    "// kernel 定义\n",
    "\n",
    "\\__global\\__ void VecAdd（float \\*A, float \\*B, float \\*C）{\n",
    "\n",
    "}\n",
    "\n",
    "int main(){\n",
    "    # kernel 调用\n",
    "    VecAdd<<<1, N>>>(A, B, C);\n",
    "}\n",
    "\n",
    "\\--------------------------------------------------------------------------------------------------------\n",
    "\n",
    "- VecAdd<<<1, N>>>(A, B, C) 语句完成对内核函数VecAdd的调用， \"<<<>>>\"运算符是内核函数的执行参数，小括号里时函数的参数；1代表kernel的Grid中只有1个block, N代表每个block中有N个thread\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 thread structure\n",
    "\n",
    "### 2.3.1 软件映射\n",
    "- 为了实现透明扩展，cuda将计算任务映射为大量的可以并行执行的线程，以在拥有不同核心数量的硬件上执行\n",
    "- kernel 以线程网格（Grid）的形式组织，每个线程网格由若干线程块（block）组成，每个线程块又由若干个线程（thread）组成，kernel实际上是以block为执行单位的\n",
    "![figure 4. grid & block & thread](http://7xk3b0.com1.z0.glb.clouddn.com/grid.jpg)\n",
    "- cuda引入grid只是用来表示一系列可以被并行执行的block， 各block是并行执行的，block间无法通信，也没有执行顺序\n",
    "- 目前一个kernel函数中只有一个grid\n",
    "- cuda 使用了dim3类型的内建变量threadIdx和blcokIdx来标志线程，构成一维，二维，三维的线程块：\n",
    "    一维block： 线程threadID为threadIdx.x\n",
    "    大小为（Dx, Dy）的二维block： 线程threadID为（threadIdx.x + threadIdx.y \\* Dx） （列优先）\n",
    "    大小为（Dx, Dy, Dz）的三维block: 线程threadID为（threadIdx.x + threadIdx.y \\* Dx + threadIdx.z \\* Dx \\* Dy）\n",
    "- __一个blcok中的线程数量不能超过固定限制__\n",
    "- block对应与处理器核心（SM），共享所在核心的存储器\n",
    "- grid, block配置根据硬件资源\n",
    "\n",
    "\\--------------------------------------------------------------------------------------------------------\n",
    "\n",
    "// 对两个尺寸为N\\*N的矩阵A,B求和，结果存储在C中\n",
    "\n",
    "// kernel 定义\n",
    "\n",
    "\\__global\\__ void MatAdd0（float A[N][N], float B[N][N], float C[N][N]）{\n",
    "\n",
    "    int i = threadIdx.x;\n",
    "    int j = threadIdx.y;\n",
    "    C[i][j] = A[i][j] + B[i][j];\n",
    "\n",
    "\\__global\\__ void MatAdd（float A[N][N], float B[N][N], float C[N][N]）{\n",
    "\n",
    "    int i = blockIdx.x \\* blockDim.x + threadIdx.x;\n",
    "    int j = blockIdx.y \\* blockDim.y + threadIdx.y;\n",
    "    if (i < N && j < N)\n",
    "        C[i][j] = A[i][j] + B[i][j];\n",
    "        \n",
    "}\n",
    "\n",
    "int main(){\n",
    "\n",
    "    # kernel 调用 1\n",
    "    dim3 dimBlock1（N, N）；\n",
    "    MatAdd0<<<1, dimBlock1>>>(A, B， C);\n",
    "    \n",
    "    # kernel 调用 2\n",
    "    dim3 dimBlock(16, 16);\n",
    "    dim3 dimGrid((N + dimBlock.x -1)/dimBlock.x, (N + dimBlock.y -1)/dimBlock.y);\n",
    "    MatAdd<<<dimGrid, dimBlock>>>(A, B， C);\n",
    "}\n",
    "\n",
    "\\--------------------------------------------------------------------------------------------------------\n",
    "\n",
    "- cuda中实现block内通信的方法是：在同一个block中的线程通过共享存储器（shared memory）交换数据，并通过栅栏同步保证线程间能正确同步数据， \\__syncthreads()函数实现同步\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 硬件映射\n",
    "\n",
    "\n",
    "![figure 5. streaming multiprocessor](http://7xk3b0.com1.z0.glb.clouddn.com/sm.jpg?imageView2/2/w/700/q/75)\n",
    "\n",
    "- GTX Titan X for example\n",
    "    SMs: 24, each has 128 cuda cores (SP, stream processor), total 2048 cuda cores\n",
    "    Threads: up to 2K threads per SM\n",
    "- __SP 只是执行单元，并不是完整的处理核心，完整的处理核心应该包含取指，解码，分发逻辑和执行单元等__\n",
    "- 一个block必须被分配到一个SM中，但是一个SM中同一时刻可以含有多个活动线程块在等待被执行\n",
    "- cuda编程模型中， 整个Grid被加载到流处理器阵列（SPA， Scalable Streaming Processor Array), 再将各个block分发到各个SM上\n",
    "- 硬件两层架构： SPA - TPC - SM, SPA包含若干个TPC（线程处理器群）， 每个TPC包含若干个SM\n",
    "- 实际运行中，block会被分割为更小的线程束（warp）, e.g. 在Tesla架构的GPU中，一个线程束由连续的32个线程组成\n",
    "- 在硬件中实际执行程序时，warp才是真正的执行单位\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part3\"></a>\n",
    "# 三. CUDA with C/C++\n",
    "\n",
    "## 3.1  keyword \\__global\\__ indicates a function that\n",
    "      Runs on the device\n",
    "      Is called from host code\n",
    "## 3.2  nvcc separates source code into host and device components\n",
    "      Device functions (e.g. mykernel()) processed by NVIDIA compiler\n",
    "      Host functions(e.g. main()) processed by standard host compiler (gcc, cl.exe)\n",
    "## 3.3  triple angle brackets mark a call from host code to device code\n",
    "      Also called a \"kernel launch\"\n",
    "      We'll return to parameters(1,1) in a moment\n",
    "## 3.4  that's all that is required to execute a function on the GPU!\n",
    "\n",
    "\n",
    "## references:\n",
    "- http://devblogs.nvidia.com/parallelforall/easy-introduction-cuda-c-and-c/\n",
    "- http://blog.csdn.net/abcjennifer/article/details/42436727\n",
    "- http://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#using-cuda-gpu-timers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Few sample for CUDA C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.1 simple sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-4d7c08036122>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-4d7c08036122>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    (/, hello_world.c)\u001b[0m\n\u001b[1;37m     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "// hello_world.c\n",
    "\n",
    "// host code (standard c/c++ code)\n",
    "\n",
    "#include <stdio.h>\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "    printf(\"c: Hello World!\\n\");\n",
    "    return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// hello_world.cu\n",
    "// device code\n",
    "\n",
    "#include <stdio.h>\n",
    "__global__ void mykernel(void)\n",
    "{\n",
    "\n",
    "}\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "    mykernel<<<1,1>>>();\n",
    "    printf(\"cu: Hello World!\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vector add with CPU\n",
    "\n",
    "//VecAdd.c\n",
    "\n",
    "#include <stdio.h> \n",
    "#include <stdlib.h>\n",
    "\n",
    "#define SIZE 1024\n",
    "    \n",
    "void VecAdd(int *a, int *b, int *c, int n)\n",
    "{\n",
    "    int i;\n",
    "    \n",
    "    for (i = 0; i < n; ++i)\n",
    "    {\n",
    "        c[i] = a[i] + b[i];\n",
    "    }\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "    // define a, b, c and alloc memory for them\n",
    "    int *a, *b, *c;\n",
    "    a = (int *)malloc(SIZE * sizeof(int));\n",
    "    b = (int *)malloc(SIZE * sizeof(int));\n",
    "    c = (int *)malloc(SIZE * sizeof(int));\n",
    "    \n",
    "    // initialize a, b, c\n",
    "    int i = 0;\n",
    "    \n",
    "    for (i = 0; i < SIZE; ++i)\n",
    "    {\n",
    "        a[i] = i;\n",
    "        b[i] = i;\n",
    "        c[i] = 0;\n",
    "    }\n",
    "    \n",
    "    VecAdd(a, b, c, SIZE);\n",
    "    \n",
    "    for (i = 0; i < 10; ++i)\n",
    "    {\n",
    "        printf(\"c[%d] = %d\\n\", i, c[i]);\n",
    "    }\n",
    "    \n",
    "    free(a); \n",
    "    free(b); \n",
    "    free(c);\n",
    "        \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vector add with GPU\n",
    "\n",
    "//VecAdd.cu\n",
    "\n",
    "#include <stdio.h> \n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "    \n",
    "    \n",
    "#define SIZE 1024\n",
    "\n",
    "// Kernel definition \n",
    "__global__ void VecAdd_T(int *a, int *b, int *c, int n) \n",
    "{ \n",
    "    int i = threadIdx.x; \n",
    "    \n",
    "    if (i < n)\n",
    "        c[i] = a[i] + b[i]; \n",
    "} \n",
    "\n",
    "// Kernel definition \n",
    "__global__ void VecAdd_B(int *a, int *b, int *c, int n) \n",
    "{ \n",
    "    int i = blockIdx.x; \n",
    "    \n",
    "    if (i < n)\n",
    "        c[i] = a[i] + b[i]; \n",
    "} \n",
    "\n",
    "// Kernel definition \n",
    "__global__ void VecAdd_BT(int *a, int *b, int *c, int n) \n",
    "{ \n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x; \n",
    "    \n",
    "    if (i < n)\n",
    "        c[i] = a[i] + b[i]; \n",
    "} \n",
    "\n",
    "\n",
    "int main() \n",
    "{ \n",
    "    // time for the whole process\n",
    "    clock_t start, finish; \n",
    "    float time;\n",
    "    start = clock(); \n",
    "    \n",
    "    // define a, b, c and alloc memory for them\n",
    "    int *a, *b, *c;\n",
    "    \n",
    "    a = (int *)malloc(SIZE * sizeof(int));\n",
    "    b = (int *)malloc(SIZE * sizeof(int));\n",
    "    c = (int *)malloc(SIZE * sizeof(int));\n",
    "    \n",
    "    // define d_a, d_b, d_c and alloc memory for them\n",
    "    int *d_a, *d_b, *d_c;\n",
    "    \n",
    "    cudaMalloc( &d_a, SIZE * sizeof(int)); // global memory on device\n",
    "    cudaMalloc( &d_b, SIZE * sizeof(int));\n",
    "    cudaMalloc( &d_c, SIZE * sizeof(int));\n",
    "    \n",
    "    // initialize a, b, c\n",
    "    int i = 0;\n",
    "    \n",
    "    for (i = 0; i < SIZE; ++i)\n",
    "    {\n",
    "        a[i] = i;\n",
    "        b[i] = i;\n",
    "        c[i] = 0;\n",
    "    }\n",
    "    \n",
    "    \n",
    "    // copy data from host memory to device memory\n",
    "    cudaMemcpy( d_a, a, SIZE * sizeof(int), cudaMemcpyHostToDevice );\n",
    "    cudaMemcpy( d_b, b, SIZE * sizeof(int), cudaMemcpyHostToDevice );\n",
    "    cudaMemcpy( d_c, c, SIZE * sizeof(int), cudaMemcpyHostToDevice );\n",
    "      \n",
    "    //----------------------------------------------------------------\n",
    "    \n",
    "    /*\n",
    "    cudaEvent_t start_cu, stop_cu;\n",
    "    float time_gpu;\n",
    "    cudaEventCreate(&start_cu);\n",
    "    cudaEventCreate(&stop_cu);\n",
    "    cudaEventRecord( start_cu, 0);\n",
    "    */\n",
    "    \n",
    "    // Kernel invocation with N threads \n",
    "    VecAdd_T<<<1, SIZE>>>(d_a, d_b, d_c, SIZE);\n",
    "    \n",
    "    // !!! number of theads per block was limit, 1024 for example, if SIZE > 1024, no error when complile, but the results was wrong \n",
    "    \n",
    "    // Kernel invocation with N blocks \n",
    "    //VecAdd_B<<<SIZE, 1>>>(d_a, d_b, d_c, SIZE); \n",
    "    \n",
    "    // Kernel invocation with m blocks and n threads;\n",
    "    //int threadsPerBlock = 256;\n",
    "    //int blocksPerGrid = (SIZE + threadsPerBlock - 1) / threadsPerBlock;\n",
    "    //VecAdd_BT<<<blocksPerGrid, threadsPerBlock>>>(d_a, d_b, d_c, SIZE);\n",
    "\n",
    "    /*\n",
    "    cudaEventRecord( stop_cu, 0);\n",
    "    cudaEventSynchronize( stop_cu );\n",
    "    cudaEventElapsedTime( &time_gpu, start_cu, stop_cu );\n",
    "    cudaEventDestroy( start_cu );\n",
    "    cudaEventDestroy( stop_cu );\n",
    "    */\n",
    "    \n",
    "    // copy results form device memory to host memory\n",
    "    cudaMemcpy( c, d_c, SIZE * sizeof(int), cudaMemcpyDeviceToHost );\n",
    "    //------------------------------------------------------------------\n",
    "    \n",
    "    for (i = 0; i < 10; ++i)\n",
    "    {\n",
    "        printf(\"c[%d] = %d\\n\", i, c[i]);\n",
    "    }\n",
    "    \n",
    "    //printf(\"calculation time_gpu = %fms\\n\", time_gpu);\n",
    "    // free space\n",
    "    free(a); \n",
    "    free(b); \n",
    "    free(c);\n",
    "    \n",
    "    cudaFree(d_a);\n",
    "    cudaFree(d_b);\n",
    "    cudaFree(d_c);\n",
    "    \n",
    "    finish = clock();  \n",
    "    time = (float)(finish - start) / CLOCKS_PER_SEC;\n",
    "    printf(\"calculation time = %fms\\n\", time);\n",
    "    return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.5.2 Matrix Add\n",
    "[MatAdd.cu](./MatAdd.c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.3 cudaMalloc for 2D array \n",
    "[2D and 3D malloc](http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory)\n",
    "\n",
    "### 3.5.4 __Notes: 不同架构的GPU的block 中thread数是有限制的，设置超过限制时，编译没有错误，但计算结果缺少或者不正确__\n",
    "\n",
    "### 3.5.5 CUDA Pro Tip: Flexible kernels with grid-stride loops\n",
    "\n",
    "[Reference: flexible kernels](http://devblogs.nvidia.com/parallelforall/cuda-pro-tip-write-flexible-kernels-grid-stride-loops/)\n",
    "\n",
    "#### Common CUDA guidance is to launch on thread per data element, so, we need enough threads to cover array or matrix size, I'll refer to this style of kernel as a monolithic kernel.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.6 shared memory for matrix multiplication\n",
    "[reference](http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#shared-memory)\n",
    "\n",
    "## $$A_{A.height,A.width} * B_{B.height,B.width} = C_{A.height,B.width}$$\n",
    "## $$A_{m,n} * B_{n,p} = C_{m,p}$$\n",
    "\n",
    "#### 1). straightforward implementation\n",
    "\n",
    "<a id=\"figure6\"></a>\n",
    "![figure 6. straightforward multiplication](http://7xk3b0.com1.z0.glb.clouddn.com/A_B.jpg?imageView2/2/w/700/q/75)\n",
    "\n",
    "Each thread reads one row of A and one column of B and computes the corresponding element of C as illustrated in [Figure 6](#figure6). A is therefore read B.width times from global memory and B is read A.height times.\n",
    "\n",
    "#### 2). implementation with shared memory\n",
    "\n",
    "<a id=\"figure7\"></a>\n",
    "![figure 6. straightforward multiplication](http://7xk3b0.com1.z0.glb.clouddn.com/A_B_s.jpg?imageView2/2/w/700/q/75)\n",
    "\n",
    "In this implementation, each thread block is responsible for computing one square sub-matrix Csub of C and each thread within the block is responsible for computing one element of Csub. As illustrated in [Figure 7](#figure7);\n",
    "\n",
    "具体实现：block（0,0）需要由A的一行tile和B的一行tile计算得到，对于每个block：\n",
    "\n",
    "    first loop: 取出A的tile0，B的tile0',放入shared memory， 计算得到Csub\n",
    "    \n",
    "    second loop: 取出A的tile1，B的tile1',放入shared memory， 在原来Csub基础上加和得到新的Csub，Csub即C中对应block的值\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    如果A的一行有多个tile，则依次类推\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part4\"></a>\n",
    "# 四. CUDA with Python\n",
    "\n",
    "\n",
    "### references\n",
    "- http://documen.tician.de/pycuda/tutorial.html\n",
    "- http://documen.tician.de/pycuda/\n",
    "- http://wiki.tiker.net/PyCuda/Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 8192 threads...\n",
      "Calculation 100000 iterations...\n",
      "\n",
      "\n",
      "SourceModule time and first three results:\n",
      "0.006603, [ 0.01912751  0.01912751  0.01912751]\n",
      "\n",
      "\n",
      "Elementwise time and first three results:\n",
      "0.181595, [ 0.005477  0.005477  0.005477]\n",
      "\n",
      "\n",
      "GPUArray time and first three results:\n",
      "4.537719, [ 0.005477  0.005477  0.005477]\n"
     ]
    }
   ],
   "source": [
    "# iterate sin function use GPU acceleration\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.tools\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule\n",
    "import pycuda.gpuarray as gpuarray\n",
    "import pycuda.cumath\n",
    "from pycuda.elementwise import ElementwiseKernel\n",
    "\n",
    "blocks = 64\n",
    "block_size = 128\n",
    "nbr_values = blocks * block_size\n",
    "n_iter = 100000\n",
    "\n",
    "print \"Using %d threads...\" % nbr_values\n",
    "print \"Calculation %d iterations...\" % n_iter\n",
    "\n",
    "# create two timers to test the speed \n",
    "start = cuda.Event()\n",
    "stop = cuda.Event()\n",
    "\n",
    "############################################################\n",
    "# SourceModule approach\n",
    "# write cuda c code and compile it two ptx with SourceModule\n",
    "mod = SourceModule(\"\"\"\n",
    "__global__ void gpusin(float *dest, float *a, int n_iter)\n",
    "{\n",
    "    const int i = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "    int n;\n",
    "    for (n = 0; n < n_iter; ++n)\n",
    "    {\n",
    "        a[i] = sin(a[i]);\n",
    "    }\n",
    "    dest[i] = a[i];\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "gpusin = mod.get_function(\"gpusin\")\n",
    "\n",
    "# create an array of 1D\n",
    "a = np.ones(nbr_values).astype(np.float32)\n",
    "# create an array that receive the result\n",
    "dest = np.zeros_like(a)\n",
    "\n",
    "start.record()\n",
    "gpusin(cuda.Out(dest), cuda.In(a), np.int32(nbr_values), grid=(blocks, 1), block=(block_size, 1, 1))\n",
    "stop.record()\n",
    "stop.synchronize()\n",
    "secs = start.time_till(stop)*1e-3\n",
    "\n",
    "print \"\\n\"\n",
    "print \"SourceModule time and first three results:\"\n",
    "print \"%f, %s\" % (secs, str(dest[:3]))\n",
    "\n",
    "#################################################################\n",
    "# Elementwise approach\n",
    "kernel = ElementwiseKernel(\n",
    "    \"float *a, int n_iter\",\n",
    "    \"int n; for (n = 0; n < n_iter; ++n) { a[i] = sin(a[i]); }\",\n",
    "    \"gpusin\")\n",
    "\n",
    "a = np.ones(nbr_values).astype(np.float32)\n",
    "a_gpu = gpuarray.to_gpu(a)\n",
    "\n",
    "start.record()\n",
    "kernel(a_gpu, np.int32(n_iter))\n",
    "stop.record()\n",
    "stop.synchronize()\n",
    "secs = start.time_till(stop)*1e-3\n",
    "print \"\\n\"\n",
    "print \"Elementwise time and first three results:\"\n",
    "print \"%f, %s\" % (secs, str(a_gpu[:3]))\n",
    "\n",
    "#################################################################\n",
    "# GPUArray approach\n",
    "\n",
    "a = np.ones(nbr_values).astype(np.float32)\n",
    "a_gpu = gpuarray.to_gpu(a)\n",
    "\n",
    "start.record()\n",
    "for i in xrange(n_iter):\n",
    "    a_gpu = pycuda.cumath.sin(a_gpu)\n",
    "stop.record()\n",
    "stop.synchronize()\n",
    "secs = start.time_till(stop)*1e-3\n",
    "print \"\\n\"\n",
    "print \"GPUArray time and first three results:\"\n",
    "print \"%f, %s\" % (secs, str(a_gpu[:3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C[:5] = [ 2.  2.  2.  2.  2.]\n",
      "C[-5:] = [ 2.  2.  2.  2.  2.]\n",
      "VectorAdd took 7.168293 seconds\n"
     ]
    }
   ],
   "source": [
    "# simple python code for vector add\n",
    "\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "def VectorAdd(a, b, c):\n",
    "    for i in xrange(a.size):\n",
    "        c[i] = a[i] + b[i]\n",
    "        \n",
    "def main():\n",
    "    N = 32000000\n",
    "    \n",
    "    A = np.ones(N, dtype=np.float32)\n",
    "    B = np.ones(N, dtype=np.float32)\n",
    "    C = np.zeros(N, dtype=np.float32)\n",
    "    \n",
    "    start = timer()\n",
    "    VectorAdd(A, B, C)\n",
    "    vectoradd_time = timer() - start\n",
    "    \n",
    "    print \"C[:5] = \" + str(C[:5])\n",
    "    print \"C[-5:] = \" + str(C[-5:])\n",
    "    \n",
    "    print \"VectorAdd took %f seconds\" % vectoradd_time\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# simple python code for vector add use gpu\n",
    "\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "from numbapro import vectorize\n",
    "\n",
    "@vectorize([\"float32(float32, float32)\"], target='gpu')\n",
    "def VectorAdd(a, b):\n",
    "        return a + b\n",
    "        \n",
    "def main():\n",
    "    N = 32000000\n",
    "    \n",
    "    A = np.ones(N, dtype=np.float32)\n",
    "    B = np.ones(N, dtype=np.float32)\n",
    "    C = np.zeros(N, dtype=np.float32)\n",
    "    \n",
    "    start = timer()\n",
    "    C = VectorAdd(A, B) # numbaoro automatically convert host memory to device memory\n",
    "    vectoradd_time = timer() - start\n",
    "    \n",
    "    print \"C[:5] = \" + str(C[:5])\n",
    "    print \"C[-5:] = \" + str(C[-5:])\n",
    "    \n",
    "    print \"VectorAdd took %f seconds\" % vectoradd_time\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    \n",
    "# shell command:' nvprof python VectorAdd.py ' for time compare\n",
    "# nvprof: tool that display timeline of your application's CPU and GPU activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "description": "Instant recognition with a pre-trained model and a tour of the net interface for visualizing features and parameters layer-by-layer.",
  "example_name": "Image Classification and Filter Visualization",
  "include_in_docs": true,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "priority": 1
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
