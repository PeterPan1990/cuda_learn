{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5 内存\n",
    "\n",
    "- Date: 2015-7-6\n",
    "- 杨攀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一. CPU & GPU hardware\n",
    "\n",
    "## 1.1 CPU\n",
    "\n",
    "![figure 1. workstation architecture](http://7xk3b0.com1.z0.glb.clouddn.com/cpu.jpg)\n",
    "\n",
    "### video data flow for example: \n",
    "- Source video from disk to CPU memory(through I/O hub)\n",
    "- CPU decode video, stored in CPU memory\n",
    "- GPU pull the data from CPU memory to GPU memory(through I/O hub)\n",
    "- GPU processing on the video frame (e.g. rendering), stored on GPU memory\n",
    "- Preview on displays or transfered to CPU memory for encode or storage\n",
    "\n",
    "### CPU 主要负责逻辑性较强的运算，其设计目标是使得执行单元能够以低延迟获得数据和指令，因此采用了复杂的控制逻辑和分支预测。\n",
    "\n",
    "## 1.2 GPU\n",
    "\n",
    "![figure 2. gpu architecture](http://7xk3b0.com1.z0.glb.clouddn.com/gpu.jpg)\n",
    "\n",
    "### part explanation of gpu\n",
    "- A host interface: connect to PCIe bus, communicates with host CPU (Copy Engine)\n",
    "- Giga Thread: the thread scheduler, creates threads in hardware and distribute work to cores\n",
    "- DRAM: dynamic random access memory\n",
    "\n",
    "### GPU 设计目标时在有限的面积实现很强的计算能力和很高的存储器带宽。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二. Introduction to CUDA\n",
    "\n",
    "## 2.1 CUDA 简介\n",
    "\n",
    "### 2.1.1 GPGPU \n",
    "- General Purpose Graphics Processing Unit\n",
    "- APIs： \n",
    "     CUDA(for nvidia GPUs only， 2007)\n",
    "     OpenCL\n",
    "     OpenACC\n",
    "### 2.1.2 what is cuda? [nvidia blogs]( http://blogs.nvidia.com/blog/2012/09/10/what-is-cuda-2/)\n",
    "- Compute Unified Device Architecture, 计算统一设备架构\n",
    "\n",
    "### 2.1.3 setup cuda\n",
    "- ubuntu cuda, [install](http://wiki.ubuntu.org.cn/NVIDIA)\n",
    "- pycuda：provides a Python interface to nvidia's CUDA API\n",
    "\n",
    "### 2.1.4 Simple Processing Flow\n",
    "- Copy input data from cpu memory to GPU memory\n",
    "- Load GPU program and execute, caching data on chip for performance\n",
    "- Copy results back to CPU memory \n",
    "\n",
    "\n",
    "\n",
    "## 2.2 CUDA 编程模型\n",
    "- 主机（cpu）与设备（gpu）\n",
    "- kernel 函数： 运行在gpu上的cuda并行计算函数\n",
    "- 一个完整的cuda函数由一系列的设备端kernel函数__并行__步骤和主机端的__串行__步骤共同组成\n",
    "- 一个kernel函数中存在两个层次的并行，Grid中的block之间的并行和block中的thread间并行，两层模型是cuda最重要的创新之一\n",
    "![figure 3. cuda programming model](http://7xk3b0.com1.z0.glb.clouddn.com/cudamodel.png)\n",
    "\n",
    "\\--------------------------------------------------------------------------------------------------------\n",
    "\n",
    "// kernel 定义\n",
    "\n",
    "\\__global\\__ void VecAdd（float \\*A, float \\*B, float \\*C）{\n",
    "\n",
    "}\n",
    "\n",
    "int main(){\n",
    "    # kernel 调用\n",
    "    VecAdd<<<1, N>>>(A, B, C);\n",
    "}\n",
    "\n",
    "\\--------------------------------------------------------------------------------------------------------\n",
    "\n",
    "- VecAdd<<<1, N>>>(A, B, C) 语句完成对内核函数VecAdd的调用， \"<<<>>>\"运算符是内核函数的执行参数，小括号里时函数的参数；1代表kernel的Grid中只有1个block, N代表每个block中有N个thread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 thread structure\n",
    "\n",
    "### 2.3.1 软件映射\n",
    "- 为了实现透明扩展，cuda将计算任务映射为大量的可以并行执行的线程，以在拥有不同核心数量的硬件上执行\n",
    "- kernel 以线程网格（Grid）的形式组织，每个线程网格由若干线程块（block）组成，每个线程块又由若干个线程（thread）组成，kernel实际上是以block为执行单位的\n",
    "![figure 4. grid & block & thread](http://7xk3b0.com1.z0.glb.clouddn.com/grid.jpg)\n",
    "- cuda引入grid只是用来表示一系列可以被并行执行的block， 各block是并行执行的，block间无法通信，也没有执行顺序\n",
    "- 目前一个kernel函数中只有一个grid\n",
    "- cuda 使用了dim3类型的内建变量threadIdx和blcokIdx来标志线程，构成一维，二维，三维的线程块：\n",
    "    一维block： 线程threadID为threadIdx.x\n",
    "    大小为（Dx, Dy）的二维block： 线程threadID为（threadIdx.x + threadIdx.y \\* Dx） （列优先）\n",
    "    大小为（Dx, Dy, Dz）的三维block: 线程threadID为（threadIdx.x + threadIdx.y \\* Dx + threadIdx.z \\* Dx \\* Dy）\n",
    "- __一个blcok中的线程数量不能超过512__\n",
    "\n",
    "\\--------------------------------------------------------------------------------------------------------\n",
    "\n",
    "// 对两个尺寸为N\\*N的矩阵A,B求和，结果存储在C中\n",
    "\n",
    "// kernel 定义\n",
    "\n",
    "\\__global\\__ void MatAdd0（float A[N][N], float B[N][N], float C[N][N]）{\n",
    "\n",
    "    int i = threadIdx.x;\n",
    "    int j = threadIdx.y;\n",
    "    C[i][j] = A[i][j] + B[i][j];\n",
    "\n",
    "\\__global\\__ void MatAdd（float A[N][N], float B[N][N], float C[N][N]）{\n",
    "\n",
    "    int i = blockIdx.x \\* blockDim.x + threadIdx.x;\n",
    "    int j = blockIdx.y \\* blockDim.y + threadIdx.y;\n",
    "    if (i < N && j < N)\n",
    "        C[i][j] = A[i][j] + B[i][j];\n",
    "        \n",
    "}\n",
    "\n",
    "int main(){\n",
    "\n",
    "    # kernel 调用 1\n",
    "    dim3 dimBlock1（N, N）；\n",
    "    MatAdd0<<<1, dimBlock1>>>(A, B， C);\n",
    "    \n",
    "    # kernel 调用 2\n",
    "    dim3 dimBlock(16, 16);\n",
    "    dim3 dimGrid((N + dimBlock.x -1)/dimBlock.x, (N + dimBlock.y -1)/dimBlock.y);\n",
    "    MatAdd<<<dimGrid, dimBlock>>>(A, B， C);\n",
    "}\n",
    "\n",
    "\\--------------------------------------------------------------------------------------------------------\n",
    "\n",
    "- cuda中实现block内通信的方法是：在同一个block中的线程通过共享存储器（shared memory）交换数据，并通过栅栏同步保证线程间能正确同步数据， \\__syncthreads()函数实现同步\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 硬件映射\n",
    "\n",
    "\n",
    "![figure 5. streaming multiprocessor](http://7xk3b0.com1.z0.glb.clouddn.com/sm.jpg)\n",
    "\n",
    "- GTX Titan X for example\n",
    "    SMs: 24, each has 128 cuda cores (SP, stream processor), total 2048 cuda cores\n",
    "    Threads: up to 2K threads per SM\n",
    "- __SP 只是执行单元，并不是完整的处理核心，完整的处理核心应该包含取指，解码，分发逻辑和执行单元等__\n",
    "- 一个block必须被分配到一个SM中，但是一个SM中同一时刻可以含有多个活动线程块在等待被执行\n",
    "- 实际运行中，block会被分割为更小的线程束（warp）, e.g. 在Tesla架构的GPU中，一个线程束由连续的32个线程组成\n",
    "- 在硬件中实际执行程序时，warp才是真正的执行单位\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三. CUDA with C/C++\n",
    "\n",
    "## 1. keyword \\__global\\__ indicates a function that\n",
    "      Runs on the device\n",
    "      Is called from host code\n",
    "## 2. nvcc separates source code into host and device components\n",
    "      Device functions (e.g. mykernel()) processed by NVIDIA compiler\n",
    "      Host functions(e.g. main()) processed by standard host compiler (gcc, cl.exe)\n",
    "## 3. triple angle brackets mark a call from host code to device code\n",
    "      Also called a \"kernel launch\"\n",
    "      We'll return to parameters(1,1) in a moment\n",
    "## 4. that's all that is required to execute a function on the GPU!\n",
    "\n",
    "\n",
    "## references:\n",
    "- http://devblogs.nvidia.com/parallelforall/easy-introduction-cuda-c-and-c/\n",
    "- http://blog.csdn.net/abcjennifer/article/details/42436727\n",
    "- http://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#using-cuda-gpu-timers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# simple c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nvcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 四. CUDA with Python\n",
    "\n",
    "## 1. \n",
    "## 2.\n",
    "\n",
    "## references\n",
    "- http://documen.tician.de/pycuda/tutorial.html\n",
    "- http://documen.tician.de/pycuda/\n",
    "- http://wiki.tiker.net/PyCuda/Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule\n",
    "\n",
    "print 'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "description": "Instant recognition with a pre-trained model and a tour of the net interface for visualizing features and parameters layer-by-layer.",
  "example_name": "Image Classification and Filter Visualization",
  "include_in_docs": true,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  },
  "priority": 1
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
